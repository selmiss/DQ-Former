# Model Configuration for Description-Guided Molecule Design Finetuning
# These parameters define the model architecture
# NOTE: This task uses TEXT input (no graph encoder needed)

# Pretrained checkpoint path (Stage 2 checkpoint for molecule design training)
model_name_or_path: unsloth/Llama-3.1-8B-Instruct # Automatically loads Stage 2 full model checkpoint

baseline_type: mollama
lora_path: ${HF_HOME}/hub/models--DongkiKim--Mol-Llama-3.1-8B-Instruct/snapshots/dd99e6ea328e01d713ac31aaad017074a6126483/model.safetensors
lora_init: True
# QFormer Configuration
use_flash_attention: True
use_dq_encoder: True  # No graph encoder for text-to-molecule task
num_query_tokens: 32
embed_dim: 512
cross_attention_freq: 1

# Graph Encoder Configuration
local_q_only: False

# Blending Module Configuration
enable_blending: True  # No blending needed for text-only input
num_layers: 8
num_heads: 8

# LLM Configuration
llm_backbone: null  # Optional: override default LLM model (e.g., "meta-llama/Meta-Llama-3-8B")
freeze_llm: False  # For molecule design finetuning, we typically don't freeze LLM
enable_lora_qformer: False
llm_only: True  # TEXT-ONLY MODE: Skip encoder entirely (saves ~20-25 GB GPU memory for text-to-molecule tasks)

# Training-specific model settings
tune_gnn: False
temperature: 0.1
enable_flash: True  # Flash attention for LLM
brics_gids_enable: True  # No BRICS for text input
entropy_gids_enable: True  # No entropy for text input

# Zero-shot evaluation mode
# If True: Load checkpoint, skip training, evaluate directly (requires model_name_or_path)
# If False: Normal training mode (load checkpoint, train, then evaluate)
zero_shot: False


