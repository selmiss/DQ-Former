# Training Configuration for Molecule Generation/Captioning (HuggingFace TrainingArguments compatible)

# Experiment naming
run_name: molgen_dqw2d

# Training hyperparameters
num_train_epochs: 10
learning_rate: 1.0e-4  # Initial learning rate for molecule generation
weight_decay: 0.05
warmup_steps: 100
lr_scheduler_type: "cosine"  # Must be quoted string

# Batch size and accumulation
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 32  # Effective batch size = 4 * num_gpus * 32

# Precision
bf16: True
fp16: False

# Checkpointing and evaluation
save_strategy: "epoch"
save_steps: 10  # save every N epochs
save_total_limit: 3
eval_strategy: "epoch"  # Evaluate every epoch for molecule generation
load_best_model_at_end: True
metric_for_best_model: "eval_loss"  # For generation tasks
greater_is_better: False  # False for loss (lower is better)

# Logging
logging_steps: 10
report_to:
  - wandb

# DataLoader settings
dataloader_num_workers: 4  # Reduced to avoid multiprocessing issues with complex data
dataloader_persistent_workers: False  # Disabled to prevent worker hanging
dataloader_pin_memory: True
dataloader_drop_last: True
dataloader_prefetch_factor: null
remove_unused_columns: False  # IMPORTANT: Keep our custom columns

# Distributed training
ddp_find_unused_parameters: False

# Reproducibility
seed: 0
data_seed: 0

# Note: deepspeed config path is set automatically by the training script
# based on deepspeed_stage value (2 or 3) in the script itself


