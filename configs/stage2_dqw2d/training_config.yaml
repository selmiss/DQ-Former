# Training Configuration for Stage 2 / Finetuning (HuggingFace TrainingArguments compatible)

# Experiment naming
run_name: stage2_dqw2d

# Training hyperparameters
num_train_epochs: 2
learning_rate: 1.0e-4
weight_decay: 0.05
warmup_steps: 1000
lr_scheduler_type: "cosine"  # Must be quoted string

# Batch size and accumulation
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 8

# Precision
bf16: True
fp16: False

# Checkpointing and evaluation (no eval for finetuning)
save_strategy: "epoch"
save_steps: 1  # save every N epochs
save_total_limit: null
eval_strategy: "no"  # No evaluation for finetuning (must be quoted!)
load_best_model_at_end: False

# Logging
logging_steps: 10
report_to:
  - wandb

# DataLoader settings
dataloader_num_workers: 8
dataloader_persistent_workers: True
dataloader_pin_memory: True
dataloader_drop_last: True
dataloader_prefetch_factor: 2
remove_unused_columns: False

# Distributed training
ddp_find_unused_parameters: False

# Reproducibility
seed: 0
data_seed: 0

# Note: deepspeed config path is set automatically by the training script
# based on deepspeed_stage value (2 or 3) in the script itself

